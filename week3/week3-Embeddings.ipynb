{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "# download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. `GoogleNews-vectors-negative300.bin.gz` will be downloaded in `download_week3_resources()`.\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format(fname = 'GoogleNews-vectors-negative300.bin.gz',binary = True, limit = 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    \n",
    "    mean_vect = np.zeros(dim)\n",
    "    count = 0\n",
    "    for word in question.split():\n",
    "        if word in embeddings:\n",
    "            mean_vect += embeddings[word]\n",
    "            count +=1\n",
    "            \n",
    "    if(count>0):\n",
    "        mean_vect /= count\n",
    "    \n",
    "    return mean_vect\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.019293891059\n",
      "-0.0287272135417\n",
      "0.0460561116536\n",
      "0.0852593315972\n",
      "0.0243055555556\n",
      "-0.0729031032986\n",
      "0.0...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    return sum([1 for dup in dup_ranks if dup <= k])/len(dup_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    return sum([1/(np.log2(1+dup)) for dup in dup_ranks if dup <= k])/len(dup_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should read *validation* corpus, located at `data/validation.tsv`. You will use it later to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    \n",
    "#     _, idx = np.unique(candidates, return_index=True)\n",
    "#     candidates = candidates[np.sort(idx)]\n",
    "    \n",
    "    similarities = np.zeros(len(candidates))\n",
    "    for i,cand in enumerate(candidates):\n",
    "        similarities[i] = cosine_similarity(question_to_vec(question, embeddings, dim).reshape(1, dim), question_to_vec(cand, embeddings, dim).reshape(1, dim))\n",
    "    \n",
    "    similarities = np.argsort(-similarities)\n",
    "    \n",
    "    _, idx = np.unique(similarities, return_index=True)\n",
    "    similarities = similarities[np.sort(idx)]\n",
    "    \n",
    "    return [(sim, candidates[sim]) for sim in similarities]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "#         print(ranks)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n"
     ]
    }
   ],
   "source": [
    "wv_ranking = []\n",
    "count=0\n",
    "for line in validation:\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.212 | Hits@   1: 0.212\n",
      "DCG@   5: 0.267 | Hits@   5: 0.315\n",
      "DCG@  10: 0.282 | Hits@  10: 0.363\n",
      "DCG@ 100: 0.320 | Hits@ 100: 0.552\n",
      "DCG@ 500: 0.353 | Hits@ 500: 0.811\n",
      "DCG@1000: 0.373 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n"
     ]
    }
   ],
   "source": [
    "prepared_validation = []\n",
    "count = 0\n",
    "for line in validation:\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "    prepared_validation.append([text_prepare(i) for i in line])\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n"
     ]
    }
   ],
   "source": [
    "wv_prepared_ranking = []\n",
    "count = 0\n",
    "for line in prepared_validation:\n",
    "    if count%100 ==0:\n",
    "        print(count)\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.310 | Hits@   1: 0.310\n",
      "DCG@   5: 0.380 | Hits@   5: 0.443\n",
      "DCG@  10: 0.397 | Hits@  10: 0.494\n",
      "DCG@ 100: 0.430 | Hits@ 100: 0.661\n",
      "DCG@ 500: 0.452 | Hits@ 500: 0.835\n",
      "DCG@1000: 0.470 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file('data/train.tsv','data/prepared_train.tsv')\n",
    "prepare_file('data/test.tsv','data/prepared_test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = 'data/prepared_test.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour. The size of the embeddings' dictionary should be approximately 100 000 (number of lines in the result file). If you got significantly more than this number, try to check all the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/prepared_train.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/prepared_train.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: 0.040040  loss: 0.009032  eta: 0h11m  tot: 0h2m52s  (20.0%)  lr: 0.050000  loss: 0.068925  eta: 0h15m  tot: 0h0m0s  (0.1%)1.1%  lr: 0.049940  loss: 0.053752  eta: 0h15m  tot: 0h0m1s  (0.2%)1.7%  lr: 0.049920  loss: 0.044858  eta: 0h14m  tot: 0h0m3s  (0.3%)2.6%  lr: 0.049800  loss: 0.037434  eta: 0h15m  tot: 0h0m4s  (0.5%)3.4%  lr: 0.049730  loss: 0.033680  eta: 0h15m  tot: 0h0m6s  (0.7%)7.3%  lr: 0.049259  loss: 0.024542  eta: 0h14m  tot: 0h0m13s  (1.5%)8.6%  lr: 0.049159  loss: 0.023296  eta: 0h14m  tot: 0h0m15s  (1.7%)10.2%  lr: 0.048919  loss: 0.021694  eta: 0h14m  tot: 0h0m18s  (2.0%)11.1%  lr: 0.048879  loss: 0.021008  eta: 0h14m  tot: 0h0m19s  (2.2%)11.7%  lr: 0.048839  loss: 0.020667  eta: 0h14m  tot: 0h0m20s  (2.3%)%  lr: 0.048759  loss: 0.020315  eta: 0h14m  tot: 0h0m21s  (2.4%)%  lr: 0.048398  loss: 0.018394  eta: 0h14m  tot: 0h0m28s  (3.2%)18.4%  lr: 0.048108  loss: 0.017289  eta: 0h14m  tot: 0h0m32s  (3.7%)%  lr: 0.048098  loss: 0.017241  eta: 0h14m  tot: 0h0m32s  (3.7%)22.2%  lr: 0.047768  loss: 0.016019  eta: 0h14m  tot: 0h0m39s  (4.4%)22.3%  lr: 0.047768  loss: 0.015998  eta: 0h13m  tot: 0h0m39s  (4.5%)23.3%  lr: 0.047708  loss: 0.015762  eta: 0h13m  tot: 0h0m40s  (4.7%)26.5%  lr: 0.047387  loss: 0.014989  eta: 0h13m  tot: 0h0m46s  (5.3%)27.2%  lr: 0.047327  loss: 0.014832  eta: 0h13m  tot: 0h0m47s  (5.4%)27.6%  lr: 0.047287  loss: 0.014741  eta: 0h13m  tot: 0h0m48s  (5.5%)27.7%  lr: 0.047287  loss: 0.014732  eta: 0h13m  tot: 0h0m48s  (5.5%)28.4%  lr: 0.047217  loss: 0.014574  eta: 0h13m  tot: 0h0m49s  (5.7%)29.9%  lr: 0.047107  loss: 0.014328  eta: 0h13m  tot: 0h0m52s  (6.0%)36.0%  lr: 0.046567  loss: 0.013364  eta: 0h13m  tot: 0h1m3s  (7.2%)37.3%  lr: 0.046436  loss: 0.013148  eta: 0h13m  tot: 0h1m5s  (7.5%)39.5%  lr: 0.046206  loss: 0.012893  eta: 0h13m  tot: 0h1m8s  (7.9%)40.9%  lr: 0.046106  loss: 0.012686  eta: 0h13m  tot: 0h1m11s  (8.2%)41.9%  lr: 0.046016  loss: 0.012559  eta: 0h13m  tot: 0h1m12s  (8.4%)42.1%  lr: 0.045996  loss: 0.012515  eta: 0h13m  tot: 0h1m13s  (8.4%)45.2%  lr: 0.045666  loss: 0.012198  eta: 0h13m  tot: 0h1m18s  (9.0%)45.6%  lr: 0.045626  loss: 0.012143  eta: 0h13m  tot: 0h1m19s  (9.1%)46.2%  lr: 0.045536  loss: 0.012097  eta: 0h13m  tot: 0h1m20s  (9.2%)47.4%  lr: 0.045425  loss: 0.011972  eta: 0h13m  tot: 0h1m22s  (9.5%)48.3%  lr: 0.045345  loss: 0.011908  eta: 0h13m  tot: 0h1m24s  (9.7%)48.4%  lr: 0.045305  loss: 0.011877  eta: 0h13m  tot: 0h1m24s  (9.7%)49.8%  lr: 0.045145  loss: 0.011761  eta: 0h13m  tot: 0h1m26s  (10.0%)50.1%  lr: 0.045115  loss: 0.011727  eta: 0h13m  tot: 0h1m27s  (10.0%)0.045025  loss: 0.011662  eta: 0h12m  tot: 0h1m28s  (10.2%)52.3%  lr: 0.044945  loss: 0.011575  eta: 0h12m  tot: 0h1m30s  (10.5%)52.4%  lr: 0.044925  loss: 0.011566  eta: 0h12m  tot: 0h1m31s  (10.5%)52.8%  lr: 0.044895  loss: 0.011539  eta: 0h12m  tot: 0h1m31s  (10.6%)57.0%  lr: 0.044515  loss: 0.011216  eta: 0h12m  tot: 0h1m38s  (11.4%)57.2%  lr: 0.044495  loss: 0.011211  eta: 0h12m  tot: 0h1m38s  (11.4%)0h1m42s  (11.9%)%  lr: 0.044144  loss: 0.010910  eta: 0h12m  tot: 0h1m45s  (12.2%)61.2%  lr: 0.044094  loss: 0.010875  eta: 0h12m  tot: 0h1m45s  (12.2%)62.8%  lr: 0.043884  loss: 0.010772  eta: 0h12m  tot: 0h1m48s  (12.6%) (12.6%)63.5%  lr: 0.043764  loss: 0.010728  eta: 0h12m  tot: 0h1m49s  (12.7%)63.8%  lr: 0.043694  loss: 0.010709  eta: 0h12m  tot: 0h1m50s  (12.8%)64.5%  lr: 0.043654  loss: 0.010659  eta: 0h12m  tot: 0h1m51s  (12.9%)64.6%  lr: 0.043644  loss: 0.010650  eta: 0h12m  tot: 0h1m51s  (12.9%)65.7%  lr: 0.043534  loss: 0.010577  eta: 0h12m  tot: 0h1m53s  (13.1%)66.4%  lr: 0.043444  loss: 0.010542  eta: 0h12m  tot: 0h1m55s  (13.3%)66.6%  lr: 0.043414  loss: 0.010535  eta: 0h12m  tot: 0h1m55s  (13.3%)66.7%  lr: 0.043414  loss: 0.010526  eta: 0h12m  tot: 0h1m55s  (13.3%)0.043383  loss: 0.010500  eta: 0h12m  tot: 0h1m56s  (13.4%)70.0%  lr: 0.043113  loss: 0.010312  eta: 0h12m  tot: 0h2m1s  (14.0%)70.9%  lr: 0.042983  loss: 0.010257  eta: 0h12m  tot: 0h2m2s  (14.2%)72.4%  lr: 0.042903  loss: 0.010184  eta: 0h12m  tot: 0h2m5s  (14.5%)%  lr: 0.042863  loss: 0.010151  eta: 0h12m  tot: 0h2m6s  (14.6%)73.2%  lr: 0.042793  loss: 0.010126  eta: 0h12m  tot: 0h2m6s  (14.6%)74.4%  lr: 0.042723  loss: 0.010065  eta: 0h12m  tot: 0h2m8s  (14.9%)12m  tot: 0h2m9s  (14.9%)74.9%  lr: 0.042673  loss: 0.010052  eta: 0h12m  tot: 0h2m9s  (15.0%)75.5%  lr: 0.042623  loss: 0.010021  eta: 0h12m  tot: 0h2m10s  (15.1%)76.5%  lr: 0.042573  loss: 0.009971  eta: 0h12m  tot: 0h2m12s  (15.3%)77.3%  lr: 0.042503  loss: 0.009934  eta: 0h12m  tot: 0h2m13s  (15.5%)77.4%  lr: 0.042503  loss: 0.009935  eta: 0h12m  tot: 0h2m13s  (15.5%)%  lr: 0.042443  loss: 0.009912  eta: 0h12m  tot: 0h2m14s  (15.6%)78.5%  lr: 0.042362  loss: 0.009894  eta: 0h12m  tot: 0h2m15s  (15.7%)78.9%  lr: 0.042302  loss: 0.009875  eta: 0h12m  tot: 0h2m16s  (15.8%)79.2%  lr: 0.042262  loss: 0.009850  eta: 0h12m  tot: 0h2m16s  (15.8%)79.3%  lr: 0.042262  loss: 0.009847  eta: 0h12m  tot: 0h2m17s  (15.9%)79.4%  lr: 0.042252  loss: 0.009843  eta: 0h12m  tot: 0h2m17s  (15.9%)79.8%  lr: 0.042242  loss: 0.009833  eta: 0h12m  tot: 0h2m17s  (16.0%)81.0%  lr: 0.042092  loss: 0.009785  eta: 0h12m  tot: 0h2m20s  (16.2%)81.6%  lr: 0.042052  loss: 0.009764  eta: 0h12m  tot: 0h2m21s  (16.3%)82.6%  lr: 0.041962  loss: 0.009717  eta: 0h12m  tot: 0h2m22s  (16.5%)%  lr: 0.041812  loss: 0.009664  eta: 0h11m  tot: 0h2m25s  (16.8%)84.3%  lr: 0.041782  loss: 0.009644  eta: 0h11m  tot: 0h2m25s  (16.9%)85.0%  lr: 0.041662  loss: 0.009614  eta: 0h11m  tot: 0h2m27s  (17.0%)85.7%  lr: 0.041522  loss: 0.009582  eta: 0h11m  tot: 0h2m28s  (17.1%)86.8%  lr: 0.041412  loss: 0.009541  eta: 0h11m  tot: 0h2m30s  (17.4%)87.8%  lr: 0.041291  loss: 0.009502  eta: 0h11m  tot: 0h2m31s  (17.6%)88.6%  lr: 0.041191  loss: 0.009464  eta: 0h11m  tot: 0h2m33s  (17.7%)89.3%  lr: 0.041121  loss: 0.009430  eta: 0h11m  tot: 0h2m34s  (17.9%)89.8%  lr: 0.041081  loss: 0.009403  eta: 0h11m  tot: 0h2m35s  (18.0%)90.0%  lr: 0.041051  loss: 0.009396  eta: 0h11m  tot: 0h2m35s  (18.0%)90.6%  lr: 0.040921  loss: 0.009372  eta: 0h11m  tot: 0h2m36s  (18.1%)92.5%  lr: 0.040751  loss: 0.009299  eta: 0h11m  tot: 0h2m40s  (18.5%)  lr: 0.040601  loss: 0.009255  eta: 0h11m  tot: 0h2m42s  (18.8%)95.9%  lr: 0.040401  loss: 0.009164  eta: 0h11m  tot: 0h2m46s  (19.2%)97.3%  lr: 0.040250  loss: 0.009124  eta: 0h11m  tot: 0h2m48s  (19.5%)97.5%  lr: 0.040220  loss: 0.009113  eta: 0h11m  tot: 0h2m48s  (19.5%)98.5%  lr: 0.040110  loss: 0.009073  eta: 0h11m  tot: 0h2m50s  (19.7%)99.5%  lr: 0.040090  loss: 0.009046  eta: 0h11m  tot: 0h2m52s  (19.9%)\n",
      " ---+++                Epoch    0 Train error : 0.00901837 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.030000  loss: 0.002661  eta: 0h8m  tot: 0h5m45s  (40.0%).8%  lr: 0.039660  loss: 0.002385  eta: 0h10m  tot: 0h2m58s  (20.6%)2.9%  lr: 0.039650  loss: 0.002351  eta: 0h10m  tot: 0h2m58s  (20.6%)3.7%  lr: 0.039590  loss: 0.002572  eta: 0h10m  tot: 0h3m0s  (20.7%)0.039570  loss: 0.002549  eta: 0h10m  tot: 0h3m0s  (20.8%)5.1%  lr: 0.039469  loss: 0.002563  eta: 0h10m  tot: 0h3m2s  (21.0%)5.9%  lr: 0.039359  loss: 0.002533  eta: 0h10m  tot: 0h3m3s  (21.2%)5.9%  lr: 0.039359  loss: 0.002534  eta: 0h10m  tot: 0h3m3s  (21.2%)6.8%  lr: 0.039229  loss: 0.002570  eta: 0h10m  tot: 0h3m5s  (21.4%)7.1%  lr: 0.039229  loss: 0.002536  eta: 0h10m  tot: 0h3m5s  (21.4%)7.7%  lr: 0.039189  loss: 0.002544  eta: 0h10m  tot: 0h3m6s  (21.5%)9.5%  lr: 0.038969  loss: 0.002551  eta: 0h11m  tot: 0h3m10s  (21.9%)10.2%  lr: 0.038909  loss: 0.002583  eta: 0h11m  tot: 0h3m11s  (22.0%)11.5%  lr: 0.038729  loss: 0.002593  eta: 0h11m  tot: 0h3m13s  (22.3%)14.5%  lr: 0.038408  loss: 0.002624  eta: 0h10m  tot: 0h3m18s  (22.9%)14.9%  lr: 0.038338  loss: 0.002651  eta: 0h10m  tot: 0h3m19s  (23.0%)15.8%  lr: 0.038268  loss: 0.002661  eta: 0h10m  tot: 0h3m21s  (23.2%)16.6%  lr: 0.038238  loss: 0.002643  eta: 0h10m  tot: 0h3m22s  (23.3%)17.1%  lr: 0.038208  loss: 0.002654  eta: 0h10m  tot: 0h3m23s  (23.4%)21.2%  lr: 0.037908  loss: 0.002651  eta: 0h10m  tot: 0h3m29s  (24.2%)21.6%  lr: 0.037888  loss: 0.002658  eta: 0h10m  tot: 0h3m30s  (24.3%)22.4%  lr: 0.037788  loss: 0.002649  eta: 0h10m  tot: 0h3m31s  (24.5%)23.6%  lr: 0.037668  loss: 0.002640  eta: 0h10m  tot: 0h3m33s  (24.7%)23.9%  lr: 0.037598  loss: 0.002654  eta: 0h10m  tot: 0h3m34s  (24.8%)26.7%  lr: 0.037237  loss: 0.002618  eta: 0h10m  tot: 0h3m39s  (25.3%)%  lr: 0.037217  loss: 0.002615  eta: 0h10m  tot: 0h3m40s  (25.4%)28.3%  lr: 0.037037  loss: 0.002618  eta: 0h10m  tot: 0h3m42s  (25.7%)28.4%  lr: 0.037007  loss: 0.002616  eta: 0h10m  tot: 0h3m43s  (25.7%)28.6%  lr: 0.036997  loss: 0.002610  eta: 0h10m  tot: 0h3m43s  (25.7%)28.7%  lr: 0.036997  loss: 0.002613  eta: 0h10m  tot: 0h3m43s  (25.7%)29.9%  lr: 0.036867  loss: 0.002592  eta: 0h10m  tot: 0h3m45s  (26.0%)30.2%  lr: 0.036827  loss: 0.002594  eta: 0h10m  tot: 0h3m46s  (26.0%)32.1%  lr: 0.036667  loss: 0.002587  eta: 0h10m  tot: 0h3m49s  (26.4%)32.8%  lr: 0.036617  loss: 0.002593  eta: 0h10m  tot: 0h3m50s  (26.6%)33.3%  lr: 0.036537  loss: 0.002603  eta: 0h10m  tot: 0h3m51s  (26.7%)34.0%  lr: 0.036467  loss: 0.002602  eta: 0h10m  tot: 0h3m52s  (26.8%)34.2%  lr: 0.036436  loss: 0.002605  eta: 0h10m  tot: 0h3m53s  (26.8%)34.6%  lr: 0.036406  loss: 0.002605  eta: 0h10m  tot: 0h3m53s  (26.9%)35.2%  lr: 0.036326  loss: 0.002598  eta: 0h10m  tot: 0h3m54s  (27.0%)37.2%  lr: 0.036136  loss: 0.002587  eta: 0h10m  tot: 0h3m57s  (27.4%)38.4%  lr: 0.036086  loss: 0.002615  eta: 0h10m  tot: 0h4m0s  (27.7%)39.1%  lr: 0.035986  loss: 0.002617  eta: 0h10m  tot: 0h4m1s  (27.8%)40.3%  lr: 0.035796  loss: 0.002637  eta: 0h10m  tot: 0h4m3s  (28.1%)%)42.6%  lr: 0.035566  loss: 0.002647  eta: 0h10m  tot: 0h4m8s  (28.5%)0.035526  loss: 0.002640  eta: 0h10m  tot: 0h4m8s  (28.6%)  lr: 0.035486  loss: 0.002652  eta: 0h10m  tot: 0h4m9s  (28.6%)43.9%  lr: 0.035405  loss: 0.002649  eta: 0h10m  tot: 0h4m10s  (28.8%)45.0%  lr: 0.035355  loss: 0.002636  eta: 0h10m  tot: 0h4m12s  (29.0%)45.4%  lr: 0.035325  loss: 0.002638  eta: 0h10m  tot: 0h4m12s  (29.1%)45.6%  lr: 0.035315  loss: 0.002635  eta: 0h10m  tot: 0h4m13s  (29.1%)10m  tot: 0h4m15s  (29.3%)49.0%  lr: 0.035005  loss: 0.002650  eta: 0h10m  tot: 0h4m19s  (29.8%)49.6%  lr: 0.034975  loss: 0.002642  eta: 0h10m  tot: 0h4m20s  (29.9%)%  lr: 0.034845  loss: 0.002642  eta: 0h10m  tot: 0h4m22s  (30.2%)52.5%  lr: 0.034735  loss: 0.002660  eta: 0h10m  tot: 0h4m25s  (30.5%)m  tot: 0h4m25s  (30.5%)0.034414  loss: 0.002661  eta: 0h9m  tot: 0h4m31s  (31.2%)56.4%  lr: 0.034384  loss: 0.002661  eta: 0h9m  tot: 0h4m32s  (31.3%)57.2%  lr: 0.034334  loss: 0.002670  eta: 0h9m  tot: 0h4m33s  (31.4%)%  lr: 0.034274  loss: 0.002670  eta: 0h9m  tot: 0h4m34s  (31.6%)58.2%  lr: 0.034254  loss: 0.002672  eta: 0h9m  tot: 0h4m35s  (31.6%)60.0%  lr: 0.034014  loss: 0.002658  eta: 0h9m  tot: 0h4m38s  (32.0%)61.6%  lr: 0.033864  loss: 0.002645  eta: 0h9m  tot: 0h4m41s  (32.3%)63.5%  lr: 0.033624  loss: 0.002650  eta: 0h9m  tot: 0h4m44s  (32.7%)63.6%  lr: 0.033624  loss: 0.002651  eta: 0h9m  tot: 0h4m44s  (32.7%)%  lr: 0.033534  loss: 0.002650  eta: 0h9m  tot: 0h4m46s  (32.9%)66.5%  lr: 0.033373  loss: 0.002654  eta: 0h9m  tot: 0h4m50s  (33.3%)%  lr: 0.033273  loss: 0.002654  eta: 0h9m  tot: 0h4m52s  (33.6%)68.3%  lr: 0.033233  loss: 0.002657  eta: 0h9m  tot: 0h4m53s  (33.7%)69.6%  lr: 0.033043  loss: 0.002661  eta: 0h9m  tot: 0h4m55s  (33.9%)71.0%  lr: 0.032913  loss: 0.002661  eta: 0h9m  tot: 0h4m58s  (34.2%)71.9%  lr: 0.032793  loss: 0.002658  eta: 0h9m  tot: 0h4m59s  (34.4%)72.4%  lr: 0.032733  loss: 0.002668  eta: 0h9m  tot: 0h5m0s  (34.5%)75.5%  lr: 0.032443  loss: 0.002649  eta: 0h9m  tot: 0h5m5s  (35.1%)76.2%  lr: 0.032342  loss: 0.002653  eta: 0h9m  tot: 0h5m6s  (35.2%)%  lr: 0.032282  loss: 0.002656  eta: 0h9m  tot: 0h5m8s  (35.4%)77.3%  lr: 0.032212  loss: 0.002658  eta: 0h9m  tot: 0h5m9s  (35.5%)77.9%  lr: 0.032182  loss: 0.002660  eta: 0h9m  tot: 0h5m10s  (35.6%)78.0%  lr: 0.032162  loss: 0.002665  eta: 0h9m  tot: 0h5m10s  (35.6%)78.8%  lr: 0.032132  loss: 0.002663  eta: 0h9m  tot: 0h5m11s  (35.8%)79.7%  lr: 0.032052  loss: 0.002667  eta: 0h9m  tot: 0h5m13s  (35.9%)0.032002  loss: 0.002662  eta: 0h9m  tot: 0h5m14s  (36.1%)84.1%  lr: 0.031562  loss: 0.002662  eta: 0h9m  tot: 0h5m20s  (36.8%)84.2%  lr: 0.031542  loss: 0.002662  eta: 0h9m  tot: 0h5m21s  (36.8%)87.0%  lr: 0.031241  loss: 0.002644  eta: 0h9m  tot: 0h5m25s  (37.4%)88.6%  lr: 0.031061  loss: 0.002652  eta: 0h9m  tot: 0h5m28s  (37.7%)89.8%  lr: 0.030971  loss: 0.002658  eta: 0h9m  tot: 0h5m30s  (38.0%)91.6%  lr: 0.030681  loss: 0.002660  eta: 0h8m  tot: 0h5m33s  (38.3%)%  lr: 0.030381  loss: 0.002656  eta: 0h8m  tot: 0h5m39s  (38.9%)96.1%  lr: 0.030180  loss: 0.002654  eta: 0h8m  tot: 0h5m41s  (39.2%)96.7%  lr: 0.030150  loss: 0.002660  eta: 0h8m  tot: 0h5m42s  (39.3%)97.0%  lr: 0.030130  loss: 0.002659  eta: 0h8m  tot: 0h5m42s  (39.4%)97.9%  lr: 0.030060  loss: 0.002659  eta: 0h8m  tot: 0h5m44s  (39.6%)98.2%  lr: 0.030050  loss: 0.002659  eta: 0h8m  tot: 0h5m44s  (39.6%)\n",
      " ---+++                Epoch    1 Train error : 0.00267497 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.020000  loss: 0.001926  eta: 0h5m  tot: 0h8m36s  (60.0%)  tot: 0h5m46s  (40.2%)h7m  tot: 0h5m48s  (40.4%)  loss: 0.001601  eta: 0h7m  tot: 0h5m49s  (40.5%)3.5%  lr: 0.029710  loss: 0.001695  eta: 0h7m  tot: 0h5m50s  (40.7%)3.6%  lr: 0.029700  loss: 0.001693  eta: 0h7m  tot: 0h5m51s  (40.7%)3.8%  lr: 0.029680  loss: 0.001754  eta: 0h7m  tot: 0h5m51s  (40.8%)6.1%  lr: 0.029419  loss: 0.001853  eta: 0h7m  tot: 0h5m55s  (41.2%)6.4%  lr: 0.029379  loss: 0.001849  eta: 0h7m  tot: 0h5m55s  (41.3%)%  lr: 0.029349  loss: 0.001861  eta: 0h7m  tot: 0h5m56s  (41.4%)7.1%  lr: 0.029319  loss: 0.001882  eta: 0h7m  tot: 0h5m56s  (41.4%)9.5%  lr: 0.029149  loss: 0.001894  eta: 0h7m  tot: 0h6m1s  (41.9%)10.4%  lr: 0.029069  loss: 0.001882  eta: 0h7m  tot: 0h6m2s  (42.1%)12.0%  lr: 0.028939  loss: 0.001867  eta: 0h7m  tot: 0h6m5s  (42.4%)20.9%  lr: 0.028048  loss: 0.001949  eta: 0h7m  tot: 0h6m20s  (44.2%)21.4%  lr: 0.027998  loss: 0.001944  eta: 0h7m  tot: 0h6m21s  (44.3%)24.8%  lr: 0.027708  loss: 0.001941  eta: 0h7m  tot: 0h6m27s  (45.0%)25.9%  lr: 0.027538  loss: 0.001928  eta: 0h7m  tot: 0h6m29s  (45.2%)%  lr: 0.027437  loss: 0.001926  eta: 0h7m  tot: 0h6m31s  (45.4%)28.5%  lr: 0.027277  loss: 0.001932  eta: 0h7m  tot: 0h6m34s  (45.7%)28.7%  lr: 0.027257  loss: 0.001935  eta: 0h7m  tot: 0h6m34s  (45.7%)29.7%  lr: 0.027197  loss: 0.001929  eta: 0h7m  tot: 0h6m36s  (45.9%)30.4%  lr: 0.027087  loss: 0.001931  eta: 0h7m  tot: 0h6m37s  (46.1%)31.7%  lr: 0.026937  loss: 0.001930  eta: 0h7m  tot: 0h6m39s  (46.3%)33.6%  lr: 0.026757  loss: 0.001915  eta: 0h7m  tot: 0h6m43s  (46.7%)%  lr: 0.026757  loss: 0.001916  eta: 0h7m  tot: 0h6m43s  (46.7%)34.2%  lr: 0.026707  loss: 0.001916  eta: 0h7m  tot: 0h6m44s  (46.8%)36.7%  lr: 0.026477  loss: 0.001885  eta: 0h7m  tot: 0h6m48s  (47.3%)38.3%  lr: 0.026266  loss: 0.001908  eta: 0h7m  tot: 0h6m51s  (47.7%)38.4%  lr: 0.026246  loss: 0.001907  eta: 0h7m  tot: 0h6m51s  (47.7%)7m  tot: 0h6m52s  (47.8%)42.1%  lr: 0.025966  loss: 0.001905  eta: 0h7m  tot: 0h6m58s  (48.4%)42.2%  lr: 0.025946  loss: 0.001910  eta: 0h7m  tot: 0h6m58s  (48.4%)0.025816  loss: 0.001923  eta: 0h7m  tot: 0h7m1s  (48.8%)0.025646  loss: 0.001923  eta: 0h7m  tot: 0h7m3s  (49.1%)0.025636  loss: 0.001922  eta: 0h7m  tot: 0h7m3s  (49.1%)46.8%  lr: 0.025536  loss: 0.001914  eta: 0h7m  tot: 0h7m5s  (49.4%)7m6s  (49.5%)49.3%  lr: 0.025245  loss: 0.001920  eta: 0h7m  tot: 0h7m10s  (49.9%)50.1%  lr: 0.025095  loss: 0.001922  eta: 0h7m  tot: 0h7m11s  (50.0%)50.4%  lr: 0.025015  loss: 0.001925  eta: 0h7m  tot: 0h7m12s  (50.1%)51.9%  lr: 0.024905  loss: 0.001919  eta: 0h7m  tot: 0h7m14s  (50.4%)  eta: 0h7m  tot: 0h7m15s  (50.5%)52.5%  lr: 0.024835  loss: 0.001921  eta: 0h7m  tot: 0h7m15s  (50.5%)%  lr: 0.024785  loss: 0.001921  eta: 0h7m  tot: 0h7m16s  (50.6%)54.3%  lr: 0.024655  loss: 0.001915  eta: 0h7m  tot: 0h7m18s  (50.9%)54.7%  lr: 0.024595  loss: 0.001921  eta: 0h7m  tot: 0h7m19s  (50.9%)55.1%  lr: 0.024535  loss: 0.001921  eta: 0h6m  tot: 0h7m20s  (51.0%)%  lr: 0.024294  loss: 0.001928  eta: 0h6m  tot: 0h7m23s  (51.4%)0.024144  loss: 0.001927  eta: 0h6m  tot: 0h7m26s  (51.7%)59.4%  lr: 0.024064  loss: 0.001930  eta: 0h6m  tot: 0h7m27s  (51.9%)59.5%  lr: 0.024054  loss: 0.001929  eta: 0h6m  tot: 0h7m27s  (51.9%)60.0%  lr: 0.024014  loss: 0.001931  eta: 0h6m  tot: 0h7m29s  (52.0%)61.5%  lr: 0.023854  loss: 0.001935  eta: 0h6m  tot: 0h7m31s  (52.3%)61.9%  lr: 0.023834  loss: 0.001934  eta: 0h6m  tot: 0h7m32s  (52.4%)62.7%  lr: 0.023764  loss: 0.001936  eta: 0h6m  tot: 0h7m33s  (52.5%)63.8%  lr: 0.023644  loss: 0.001939  eta: 0h6m  tot: 0h7m35s  (52.8%)63.9%  lr: 0.023634  loss: 0.001938  eta: 0h6m  tot: 0h7m35s  (52.8%)64.3%  lr: 0.023614  loss: 0.001939  eta: 0h6m  tot: 0h7m36s  (52.9%)68.9%  lr: 0.023183  loss: 0.001948  eta: 0h6m  tot: 0h7m44s  (53.8%)70.1%  lr: 0.023033  loss: 0.001940  eta: 0h6m  tot: 0h7m46s  (54.0%)72.6%  lr: 0.022803  loss: 0.001933  eta: 0h6m  tot: 0h7m50s  (54.5%)78.4%  lr: 0.022252  loss: 0.001921  eta: 0h6m  tot: 0h8m0s  (55.7%)79.7%  lr: 0.022112  loss: 0.001920  eta: 0h6m  tot: 0h8m3s  (55.9%)%  lr: 0.022022  loss: 0.001921  eta: 0h6m  tot: 0h8m3s  (56.0%)80.4%  lr: 0.022002  loss: 0.001923  eta: 0h6m  tot: 0h8m4s  (56.1%)83.2%  lr: 0.021732  loss: 0.001920  eta: 0h6m  tot: 0h8m8s  (56.6%)83.8%  lr: 0.021652  loss: 0.001922  eta: 0h6m  tot: 0h8m9s  (56.8%)85.0%  lr: 0.021522  loss: 0.001924  eta: 0h6m  tot: 0h8m11s  (57.0%)86.4%  lr: 0.021341  loss: 0.001918  eta: 0h6m  tot: 0h8m14s  (57.3%)%  lr: 0.021321  loss: 0.001919  eta: 0h6m  tot: 0h8m15s  (57.4%)87.5%  lr: 0.021261  loss: 0.001920  eta: 0h6m  tot: 0h8m16s  (57.5%)89.3%  lr: 0.021131  loss: 0.001928  eta: 0h6m  tot: 0h8m19s  (57.9%)90.3%  lr: 0.021061  loss: 0.001929  eta: 0h6m  tot: 0h8m21s  (58.1%)92.3%  lr: 0.020891  loss: 0.001931  eta: 0h5m  tot: 0h8m24s  (58.5%)93.5%  lr: 0.020731  loss: 0.001931  eta: 0h5m  tot: 0h8m26s  (58.7%)93.9%  lr: 0.020721  loss: 0.001932  eta: 0h5m  tot: 0h8m27s  (58.8%)94.2%  lr: 0.020721  loss: 0.001934  eta: 0h5m  tot: 0h8m27s  (58.8%)59.2%)%  lr: 0.020421  loss: 0.001932  eta: 0h5m  tot: 0h8m31s  (59.2%)96.4%  lr: 0.020381  loss: 0.001932  eta: 0h5m  tot: 0h8m31s  (59.3%)96.6%  lr: 0.020330  loss: 0.001931  eta: 0h5m  tot: 0h8m31s  (59.3%)96.7%  lr: 0.020290  loss: 0.001930  eta: 0h5m  tot: 0h8m32s  (59.3%)96.8%  lr: 0.020250  loss: 0.001929  eta: 0h5m  tot: 0h8m32s  (59.4%)97.9%  lr: 0.020090  loss: 0.001927  eta: 0h5m  tot: 0h8m34s  (59.6%)98.8%  lr: 0.020050  loss: 0.001925  eta: 0h5m  tot: 0h8m35s  (59.8%)\n",
      " ---+++                Epoch    2 Train error : 0.00188486 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.010000  loss: 0.001554  eta: 0h2m  tot: 0h11m26s  (80.0%)20000  loss: 0.001323  eta: 0h4m  tot: 0h8m37s  (60.1%)5m  tot: 0h8m38s  (60.3%)2.7%  lr: 0.019730  loss: 0.001544  eta: 0h5m  tot: 0h8m41s  (60.5%)4.3%  lr: 0.019500  loss: 0.001485  eta: 0h5m  tot: 0h8m44s  (60.9%)5.5%  lr: 0.019369  loss: 0.001464  eta: 0h5m  tot: 0h8m46s  (61.1%)5.6%  lr: 0.019369  loss: 0.001447  eta: 0h5m  tot: 0h8m46s  (61.1%)8.9%  lr: 0.019129  loss: 0.001475  eta: 0h5m  tot: 0h8m52s  (61.8%)11.3%  lr: 0.018849  loss: 0.001501  eta: 0h5m  tot: 0h8m56s  (62.3%)12.0%  lr: 0.018739  loss: 0.001479  eta: 0h5m  tot: 0h8m57s  (62.4%)%  lr: 0.018699  loss: 0.001479  eta: 0h5m  tot: 0h8m59s  (62.6%)13.2%  lr: 0.018689  loss: 0.001476  eta: 0h5m  tot: 0h8m59s  (62.6%)14.3%  lr: 0.018589  loss: 0.001481  eta: 0h5m  tot: 0h9m1s  (62.9%)14.4%  lr: 0.018549  loss: 0.001480  eta: 0h5m  tot: 0h9m1s  (62.9%)14.8%  lr: 0.018519  loss: 0.001485  eta: 0h5m  tot: 0h9m2s  (63.0%)14.9%  lr: 0.018519  loss: 0.001485  eta: 0h5m  tot: 0h9m2s  (63.0%)16.2%  lr: 0.018388  loss: 0.001469  eta: 0h5m  tot: 0h9m4s  (63.2%)16.3%  lr: 0.018378  loss: 0.001470  eta: 0h5m  tot: 0h9m5s  (63.3%)17.1%  lr: 0.018278  loss: 0.001476  eta: 0h5m  tot: 0h9m6s  (63.4%)18.0%  lr: 0.018168  loss: 0.001481  eta: 0h5m  tot: 0h9m8s  (63.6%)18.3%  lr: 0.018128  loss: 0.001472  eta: 0h5m  tot: 0h9m8s  (63.7%)21.5%  lr: 0.017788  loss: 0.001484  eta: 0h5m  tot: 0h9m14s  (64.3%)21.7%  lr: 0.017778  loss: 0.001483  eta: 0h5m  tot: 0h9m15s  (64.3%)22.0%  lr: 0.017738  loss: 0.001481  eta: 0h5m  tot: 0h9m15s  (64.4%)%  lr: 0.017738  loss: 0.001493  eta: 0h5m  tot: 0h9m16s  (64.4%)22.7%  lr: 0.017658  loss: 0.001493  eta: 0h5m  tot: 0h9m16s  (64.5%)24.8%  lr: 0.017417  loss: 0.001498  eta: 0h5m  tot: 0h9m20s  (65.0%)26.8%  lr: 0.017157  loss: 0.001500  eta: 0h5m  tot: 0h9m24s  (65.4%)26.9%  lr: 0.017147  loss: 0.001498  eta: 0h5m  tot: 0h9m24s  (65.4%)27.3%  lr: 0.017077  loss: 0.001505  eta: 0h5m  tot: 0h9m25s  (65.5%)30.0%  lr: 0.016817  loss: 0.001508  eta: 0h5m  tot: 0h9m29s  (66.0%)30.8%  lr: 0.016667  loss: 0.001518  eta: 0h5m  tot: 0h9m31s  (66.2%)31.1%  lr: 0.016637  loss: 0.001520  eta: 0h5m  tot: 0h9m31s  (66.2%)31.3%  lr: 0.016597  loss: 0.001516  eta: 0h4m  tot: 0h9m32s  (66.3%)32.1%  lr: 0.016517  loss: 0.001512  eta: 0h4m  tot: 0h9m33s  (66.4%)32.9%  lr: 0.016426  loss: 0.001511  eta: 0h4m  tot: 0h9m34s  (66.6%)33.1%  lr: 0.016396  loss: 0.001516  eta: 0h4m  tot: 0h9m35s  (66.6%)0.016386  loss: 0.001516  eta: 0h4m  tot: 0h9m35s  (66.7%)0.001518  eta: 0h4m  tot: 0h9m37s  (66.9%)0.016236  loss: 0.001521  eta: 0h4m  tot: 0h9m38s  (67.0%)35.9%  lr: 0.016176  loss: 0.001520  eta: 0h4m  tot: 0h9m39s  (67.2%)36.6%  lr: 0.016076  loss: 0.001518  eta: 0h4m  tot: 0h9m41s  (67.3%)%  lr: 0.016056  loss: 0.001511  eta: 0h4m  tot: 0h9m41s  (67.4%)37.1%  lr: 0.016056  loss: 0.001512  eta: 0h4m  tot: 0h9m42s  (67.4%)37.5%  lr: 0.016016  loss: 0.001510  eta: 0h4m  tot: 0h9m42s  (67.5%)0.015996  loss: 0.001502  eta: 0h4m  tot: 0h9m43s  (67.6%)40.7%  lr: 0.015756  loss: 0.001507  eta: 0h4m  tot: 0h9m48s  (68.1%)40.9%  lr: 0.015756  loss: 0.001505  eta: 0h4m  tot: 0h9m48s  (68.2%)41.2%  lr: 0.015736  loss: 0.001510  eta: 0h4m  tot: 0h9m48s  (68.2%)%  lr: 0.015726  loss: 0.001509  eta: 0h4m  tot: 0h9m49s  (68.3%)41.7%  lr: 0.015686  loss: 0.001513  eta: 0h4m  tot: 0h9m49s  (68.3%)44.6%  lr: 0.015335  loss: 0.001506  eta: 0h4m  tot: 0h9m54s  (68.9%)45.2%  lr: 0.015255  loss: 0.001506  eta: 0h4m  tot: 0h9m55s  (69.0%)46.1%  lr: 0.015165  loss: 0.001504  eta: 0h4m  tot: 0h9m57s  (69.2%)%  lr: 0.015125  loss: 0.001505  eta: 0h4m  tot: 0h9m57s  (69.3%)47.5%  lr: 0.014955  loss: 0.001509  eta: 0h4m  tot: 0h9m59s  (69.5%)49.7%  lr: 0.014715  loss: 0.001520  eta: 0h4m  tot: 0h10m3s  (69.9%)0.014475  loss: 0.001519  eta: 0h4m  tot: 0h10m8s  (70.6%)53.6%  lr: 0.014415  loss: 0.001521  eta: 0h4m  tot: 0h10m9s  (70.7%)%  lr: 0.014194  loss: 0.001521  eta: 0h4m  tot: 0h10m13s  (71.2%)58.1%  lr: 0.013954  loss: 0.001519  eta: 0h4m  tot: 0h10m17s  (71.6%)%  lr: 0.013944  loss: 0.001519  eta: 0h4m  tot: 0h10m17s  (71.6%)58.8%  lr: 0.013894  loss: 0.001516  eta: 0h4m  tot: 0h10m18s  (71.8%)59.2%  lr: 0.013874  loss: 0.001517  eta: 0h4m  tot: 0h10m19s  (71.8%)59.4%  lr: 0.013864  loss: 0.001514  eta: 0h4m  tot: 0h10m20s  (71.9%)0h4m  tot: 0h10m20s  (72.0%)60.4%  lr: 0.013804  loss: 0.001515  eta: 0h4m  tot: 0h10m21s  (72.1%)10m23s  (72.3%)62.7%  lr: 0.013604  loss: 0.001518  eta: 0h3m  tot: 0h10m25s  (72.5%)%  lr: 0.013514  loss: 0.001517  eta: 0h3m  tot: 0h10m26s  (72.7%)63.6%  lr: 0.013494  loss: 0.001517  eta: 0h3m  tot: 0h10m26s  (72.7%)63.6%  lr: 0.013484  loss: 0.001518  eta: 0h3m  tot: 0h10m26s  (72.7%)63.7%  lr: 0.013474  loss: 0.001518  eta: 0h3m  tot: 0h10m26s  (72.7%)66.1%  lr: 0.013243  loss: 0.001515  eta: 0h3m  tot: 0h10m30s  (73.2%)66.5%  lr: 0.013203  loss: 0.001517  eta: 0h3m  tot: 0h10m31s  (73.3%)67.3%  lr: 0.013183  loss: 0.001514  eta: 0h3m  tot: 0h10m32s  (73.5%)67.8%  lr: 0.013153  loss: 0.001518  eta: 0h3m  tot: 0h10m33s  (73.6%)71.7%  lr: 0.012783  loss: 0.001532  eta: 0h3m  tot: 0h10m40s  (74.3%)0.012753  loss: 0.001531  eta: 0h3m  tot: 0h10m40s  (74.4%)72.9%  lr: 0.012683  loss: 0.001529  eta: 0h3m  tot: 0h10m42s  (74.6%)73.9%  lr: 0.012563  loss: 0.001525  eta: 0h3m  tot: 0h10m43s  (74.8%)74.4%  lr: 0.012513  loss: 0.001524  eta: 0h3m  tot: 0h10m44s  (74.9%)74.8%  lr: 0.012483  loss: 0.001524  eta: 0h3m  tot: 0h10m45s  (75.0%)78.7%  lr: 0.012162  loss: 0.001533  eta: 0h3m  tot: 0h10m52s  (75.7%)80.4%  lr: 0.011972  loss: 0.001540  eta: 0h3m  tot: 0h10m55s  (76.1%)83.9%  lr: 0.011552  loss: 0.001545  eta: 0h3m  tot: 0h11m1s  (76.8%)85.2%  lr: 0.011412  loss: 0.001547  eta: 0h3m  tot: 0h11m2s  (77.0%)85.4%  lr: 0.011392  loss: 0.001547  eta: 0h3m  tot: 0h11m3s  (77.1%)0.001543  eta: 0h3m  tot: 0h11m5s  (77.3%)87.4%  lr: 0.011221  loss: 0.001544  eta: 0h3m  tot: 0h11m6s  (77.5%)3m  tot: 0h11m8s  (77.7%)89.3%  lr: 0.011031  loss: 0.001549  eta: 0h3m  tot: 0h11m9s  (77.9%)89.6%  lr: 0.011001  loss: 0.001549  eta: 0h3m  tot: 0h11m10s  (77.9%)90.7%  lr: 0.010871  loss: 0.001548  eta: 0h3m  tot: 0h11m12s  (78.1%)91.1%  lr: 0.010791  loss: 0.001555  eta: 0h3m  tot: 0h11m12s  (78.2%)92.1%  lr: 0.010731  loss: 0.001556  eta: 0h3m  tot: 0h11m14s  (78.4%)96.5%  lr: 0.010250  loss: 0.001558  eta: 0h2m  tot: 0h11m22s  (79.3%)0.010150  loss: 0.001555  eta: 0h2m  tot: 0h11m24s  (79.6%)\n",
      " ---+++                Epoch    3 Train error : 0.00154913 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.000010  loss: 0.001408  eta: <1min   tot: 0h14m17s  (100.0%) lr: 0.009920  loss: 0.001611  eta: 0h2m  tot: 0h11m28s  (80.2%)2.2%  lr: 0.009810  loss: 0.001619  eta: 0h2m  tot: 0h11m30s  (80.4%)4.2%  lr: 0.009610  loss: 0.001606  eta: 0h2m  tot: 0h11m34s  (80.8%)4.6%  lr: 0.009600  loss: 0.001643  eta: 0h2m  tot: 0h11m34s  (80.9%)m  tot: 0h11m38s  (81.4%)8.0%  lr: 0.009199  loss: 0.001522  eta: 0h2m  tot: 0h11m40s  (81.6%)10.1%  lr: 0.008999  loss: 0.001510  eta: 0h2m  tot: 0h11m44s  (82.0%)10.3%  lr: 0.008979  loss: 0.001499  eta: 0h2m  tot: 0h11m44s  (82.1%)h11m46s  (82.2%)13.2%  lr: 0.008649  loss: 0.001499  eta: 0h2m  tot: 0h11m49s  (82.6%)15.0%  lr: 0.008479  loss: 0.001507  eta: 0h2m  tot: 0h11m53s  (83.0%)15.3%  lr: 0.008438  loss: 0.001501  eta: 0h2m  tot: 0h11m53s  (83.1%)16.3%  lr: 0.008358  loss: 0.001511  eta: 0h2m  tot: 0h11m55s  (83.3%)%  lr: 0.008138  loss: 0.001503  eta: 0h2m  tot: 0h11m57s  (83.6%)18.2%  lr: 0.008118  loss: 0.001499  eta: 0h2m  tot: 0h11m58s  (83.6%)s  (83.8%)19.0%  lr: 0.008018  loss: 0.001510  eta: 0h2m  tot: 0h11m59s  (83.8%)20.5%  lr: 0.007988  loss: 0.001514  eta: 0h2m  tot: 0h12m1s  (84.1%)23.0%  lr: 0.007718  loss: 0.001514  eta: 0h2m  tot: 0h12m6s  (84.6%)23.4%  lr: 0.007658  loss: 0.001507  eta: 0h2m  tot: 0h12m7s  (84.7%)0.007327  loss: 0.001493  eta: 0h2m  tot: 0h12m13s  (85.5%)30.1%  lr: 0.007017  loss: 0.001471  eta: 0h2m  tot: 0h12m18s  (86.0%)30.4%  lr: 0.006997  loss: 0.001473  eta: 0h2m  tot: 0h12m19s  (86.1%)30.7%  lr: 0.006947  loss: 0.001472  eta: 0h2m  tot: 0h12m20s  (86.1%)30.8%  lr: 0.006937  loss: 0.001472  eta: 0h2m  tot: 0h12m20s  (86.2%)%  lr: 0.006807  loss: 0.001474  eta: 0h1m  tot: 0h12m23s  (86.5%)35.5%  lr: 0.006517  loss: 0.001460  eta: 0h1m  tot: 0h12m28s  (87.1%)1m  tot: 0h12m29s  (87.2%)37.0%  lr: 0.006376  loss: 0.001462  eta: 0h1m  tot: 0h12m30s  (87.4%)37.3%  lr: 0.006346  loss: 0.001465  eta: 0h1m  tot: 0h12m31s  (87.5%)40.0%  lr: 0.006126  loss: 0.001465  eta: 0h1m  tot: 0h12m35s  (88.0%)40.2%  lr: 0.006086  loss: 0.001460  eta: 0h1m  tot: 0h12m36s  (88.0%)42.5%  lr: 0.005796  loss: 0.001451  eta: 0h1m  tot: 0h12m40s  (88.5%)45.5%  lr: 0.005526  loss: 0.001446  eta: 0h1m  tot: 0h12m44s  (89.1%)46.4%  lr: 0.005456  loss: 0.001436  eta: 0h1m  tot: 0h12m46s  (89.3%)47.2%  lr: 0.005385  loss: 0.001441  eta: 0h1m  tot: 0h12m48s  (89.4%)0.005275  loss: 0.001439  eta: 0h1m  tot: 0h12m50s  (89.6%)49.0%  lr: 0.005165  loss: 0.001436  eta: 0h1m  tot: 0h12m52s  (89.8%)50.0%  lr: 0.005095  loss: 0.001437  eta: 0h1m  tot: 0h12m54s  (90.0%)50.4%  lr: 0.005025  loss: 0.001433  eta: 0h1m  tot: 0h12m54s  (90.1%)53.9%  lr: 0.004565  loss: 0.001425  eta: 0h1m  tot: 0h13m1s  (90.8%)58.2%  lr: 0.004164  loss: 0.001418  eta: 0h1m  tot: 0h13m8s  (91.6%)61.1%  lr: 0.003974  loss: 0.001427  eta: 0h1m  tot: 0h13m13s  (92.2%)%  lr: 0.003974  loss: 0.001426  eta: 0h1m  tot: 0h13m13s  (92.2%)63.6%  lr: 0.003724  loss: 0.001419  eta: 0h1m  tot: 0h13m17s  (92.7%)64.2%  lr: 0.003664  loss: 0.001415  eta: 0h1m  tot: 0h13m18s  (92.8%)65.4%  lr: 0.003504  loss: 0.001414  eta: 0h1m  tot: 0h13m20s  (93.1%)66.3%  lr: 0.003343  loss: 0.001414  eta: <1min   tot: 0h13m21s  (93.3%)66.8%  lr: 0.003323  loss: 0.001413  eta: <1min   tot: 0h13m22s  (93.4%)0.003293  loss: 0.001413  eta: <1min   tot: 0h13m23s  (93.4%)69.2%  lr: 0.003093  loss: 0.001413  eta: <1min   tot: 0h13m27s  (93.8%)69.9%  lr: 0.003013  loss: 0.001413  eta: <1min   tot: 0h13m28s  (94.0%)70.8%  lr: 0.002913  loss: 0.001407  eta: <1min   tot: 0h13m29s  (94.2%)13m30s  (94.3%)72.1%  lr: 0.002863  loss: 0.001410  eta: <1min   tot: 0h13m31s  (94.4%)72.2%  lr: 0.002863  loss: 0.001409  eta: <1min   tot: 0h13m31s  (94.4%)73.7%  lr: 0.002713  loss: 0.001412  eta: <1min   tot: 0h13m34s  (94.7%)73.8%  lr: 0.002713  loss: 0.001412  eta: <1min   tot: 0h13m34s  (94.8%)75.9%  lr: 0.002543  loss: 0.001409  eta: <1min   tot: 0h13m37s  (95.2%)76.1%  lr: 0.002533  loss: 0.001409  eta: <1min   tot: 0h13m38s  (95.2%)%  lr: 0.002513  loss: 0.001408  eta: <1min   tot: 0h13m38s  (95.2%)77.1%  lr: 0.002412  loss: 0.001413  eta: <1min   tot: 0h13m39s  (95.4%)77.6%  lr: 0.002352  loss: 0.001412  eta: <1min   tot: 0h13m40s  (95.5%)78.4%  lr: 0.002262  loss: 0.001415  eta: <1min   tot: 0h13m42s  (95.7%)78.8%  lr: 0.002262  loss: 0.001414  eta: <1min   tot: 0h13m42s  (95.8%)79.8%  lr: 0.002182  loss: 0.001411  eta: <1min   tot: 0h13m44s  (96.0%)0.001982  loss: 0.001412  eta: <1min   tot: 0h13m47s  (96.3%)82.0%  lr: 0.001942  loss: 0.001412  eta: <1min   tot: 0h13m48s  (96.4%)82.5%  lr: 0.001862  loss: 0.001411  eta: <1min   tot: 0h13m48s  (96.5%)86.2%  lr: 0.001371  loss: 0.001410  eta: <1min   tot: 0h13m55s  (97.2%)%  lr: 0.001311  loss: 0.001412  eta: <1min   tot: 0h13m57s  (97.4%)90.4%  lr: 0.000991  loss: 0.001419  eta: <1min   tot: 0h14m2s  (98.1%)91.1%  lr: 0.000891  loss: 0.001417  eta: <1min   tot: 0h14m4s  (98.2%)91.2%  lr: 0.000881  loss: 0.001416  eta: <1min   tot: 0h14m4s  (98.2%)91.5%  lr: 0.000861  loss: 0.001417  eta: <1min   tot: 0h14m4s  (98.3%)91.6%  lr: 0.000831  loss: 0.001416  eta: <1min   tot: 0h14m5s  (98.3%)92.3%  lr: 0.000801  loss: 0.001420  eta: <1min   tot: 0h14m6s  (98.5%)92.7%  lr: 0.000731  loss: 0.001418  eta: <1min   tot: 0h14m6s  (98.5%)14m7s  (98.6%)97.2%  lr: 0.000120  loss: 0.001412  eta: <1min   tot: 0h14m14s  (99.4%)97.9%  lr: 0.000070  loss: 0.001413  eta: <1min   tot: 0h14m15s  (99.6%)h14m16s  (99.8%)\n",
      " ---+++                Epoch    4 Train error : 0.00137580 +++--- ���\n",
      "Saving model to file : starspace_embedding\n",
      "Saving model in tsv format : starspace_embedding.tsv\n"
     ]
    }
   ],
   "source": [
    "!starspace train -trainFile \"data/prepared_train.tsv\" -model starspace_embedding -trainMode 3 \\\n",
    "-adagrad true -ngrams 1 -epoch 5 -dim 100 -similarity cosine -minCount 2 -verbose true \\\n",
    "-fileFormat labelDoc -negSearchLimit 10 -lr 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings = {}\n",
    "for line in open('starspace_embedding.tsv', encoding = 'utf-8'):\n",
    "    similar = line.strip().split('\\t')\n",
    "    starspace_embeddings[similar[0]] = np.array(similar[1:], dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n"
     ]
    }
   ],
   "source": [
    "ss_prepared_ranking = []\n",
    "count = 0\n",
    "for line in prepared_validation:\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.514 | Hits@   1: 0.514\n",
      "DCG@   5: 0.614 | Hits@   5: 0.699\n",
      "DCG@  10: 0.632 | Hits@  10: 0.757\n",
      "DCG@ 100: 0.663 | Hits@ 100: 0.906\n",
      "DCG@ 500: 0.673 | Hits@ 500: 0.980\n",
      "DCG@1000: 0.675 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 70\t64\t61\t53\t9\t67\t66\t85\t28\t95\t8\t40\t4\t56\t100\t93\t62\t86\t34\t55\t51\t16\t60\t98\t99\t29\t72\t79\t63\t2\t45\t89\t90\t57\t6...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = 'data/prepared_test.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, **don't remove** the file with these embeddings because you will need them in the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.019293891059\n",
      "-0.0287272135417\n",
      "0.0460561116536\n",
      "0.0852593315972\n",
      "0.0243055555556\n",
      "-0.0729031032986\n",
      "0.0...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n",
      "Task StarSpaceRanks: 70\t64\t61\t53\t9\t67\t66\t85\t28\t95\t8\t40\t4\t56\t100\t93\t62\t86\t34\t55\t51\t16\t60\t98\t99\t29\t72\t79\t63\t2\t45\t89\t90\t57\t6...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'kondakji.oussama@gmail.com'\n",
    "STUDENT_TOKEN = '81SNtKavCkr7ZBT8'\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
